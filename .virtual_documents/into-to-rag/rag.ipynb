





import os
from dotenv import load_dotenv

load_dotenv()

# This is the YouTube video [Jeff Bezos and Lex Fridman]
YOUTUBE_VIDEO = "https://www.youtube.com/watch?v=lB_0hR5s41Y&ab_channel=BeerBiceps"
S3_BUCKET = 'ml-dl-demo-data'





from langchain_community.chat_models import BedrockChat
from langchain_core.messages import HumanMessage

model = BedrockChat(model_id="anthropic.claude-v2", model_kwargs={"temperature": 0.1})





messages = [
    HumanMessage(
        content="Who won the ICC Criket World Cup 2019?"
    )
]

model.invoke(messages)





from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()

chain = model | parser
chain.invoke("Who won the ICC Criket World Cup 2019?")





from langchain.prompts import ChatPromptTemplate

template = """
Answer the question based on the context below. If you can't 
answer the question, reply "I don't know".

Context: {context}

Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)
prompt.format(context="Mary's sister is Susana", question="Who is Mary's sister?")





chain = prompt | model | parser
chain.invoke({
    "context": "Mary's sister is Susana",
    "question": "Who is Mary's sister?"
})





translation_prompt = ChatPromptTemplate.from_template(
                                                        "Translate {answer} to {language}"
                                                    )





from operator import itemgetter

translation_chain = (
    {"answer": chain, "language": itemgetter("language")} | translation_prompt | model | parser
)

translation_chain.invoke(
    {
        "context": "Mary's sister is Susana. She doesn't have any more siblings.",
        "question": "How many sisters does Mary have?",
        "language": "Hindi",
    }
)





from utils import transcribe_video

transcribe_video(s3_bucket_name=S3_BUCKET, youtube_video_url=YOUTUBE_VIDEO)





import json

with open("transcription.txt", "r") as file:
    transcription = json.loads(file.read())
    transcription = transcription['results']['transcripts'][0]['transcript']

transcription[:100]





len(transcription)


%%timeit

chain.invoke({"context": transcription,
              "question": "What matters when selecting a location for a business in India ?"
            })





from langchain_community.document_loaders import TextLoader

loader = TextLoader("transcription.txt")
text_documents = loader.load()

# text_documents





from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
text_splitter.split_documents(text_documents)[:5]





text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
documents = text_splitter.split_documents(text_documents)





from langchain_community.embeddings import BedrockEmbeddings

embeddings = BedrockEmbeddings()
embedded_query = embeddings.embed_query("Berlin is in Germany")

print(f"Embedding length: {len(embedded_query)}")
print(embedded_query[:10])





sentence1 = embeddings.embed_query("Welcome to Frankfurt")
sentence2 = embeddings.embed_query("This is a table")





from sklearn.metrics.pairwise import cosine_similarity

query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]
query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]

query_sentence1_similarity, query_sentence2_similarity














from langchain_community.vectorstores.pgvector import PGVector, DistanceStrategy

# Loading all env variables 
load_dotenv()

COLLECTION_NAME = 'rag-intro-on-aws'

# Connection String
CONNECTION_STRING = PGVector.connection_string_from_db_params(driver = os.getenv("PGVECTOR_DRIVER"),
                                                              user = os.getenv("PGVECTOR_USER"),                                      
                                                              password = os.getenv("PGVECTOR_PASSWORD"),                                  
                                                              host = os.getenv("PGVECTOR_HOST"),                                            
                                                              port = os.getenv("PGVECTOR_PORT"),                                          
                                                              database = os.getenv("PGVECTOR_DATABASE"),
                                                              )  

# Text Embedding model
embeddings = BedrockEmbeddings()

# Creating the VectorDB store instance   
vectorstore1 = PGVector(collection_name=COLLECTION_NAME,
                           connection_string=CONNECTION_STRING,
                           embedding_function=embeddings,
                           distance_strategy = DistanceStrategy.EUCLIDEAN,
                           use_jsonb = True
                          )


vectorstore1.add_texts([
                    "Color of the bird is red"
                    "The cat slept by the fire.",
                    "We went to the park after school.",
                    "I finished my homework early.",
                    "The bird sang a beautiful song.",
                    "She read a book before bed.",
                    "Mary has two siblings",
                    "Song was in Spanish", 
                    ])





vectorstore1.similarity_search_with_score(query="What the bird was singing", k=3)





retriever1 = vectorstore1.as_retriever()
retriever1.invoke("Whats the color of the bird who was singing?")





from langchain_core.runnables import RunnableParallel, RunnablePassthrough

setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())
setup.invoke("Whats the color of the bird who was singing?")





chain = setup | prompt | model | parser
chain.invoke("Whats the color of the bird who was singing?")





chain.invoke("Does Mary has any brother or sister ?")








from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import BedrockEmbeddings
from langchain_community.vectorstores.pgvector import PGVector
import os
from dotenv import load_dotenv

# Loading all env variables
load_dotenv()

# Load the text from the file
loader = TextLoader("transcription.txt")
documents = loader.load()

# Split the text into smaller chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)
docs = text_splitter.split_documents(documents)

# Initialize the embeddings
embeddings = BedrockEmbeddings()

# Set the collection name
COLLECTION_NAME = "rag-intro-yt"

# Connection String
CONNECTION_STRING = PGVector.connection_string_from_db_params(
    driver=os.getenv("PGVECTOR_DRIVER"),
    user=os.getenv("PGVECTOR_USER"),
    password=os.getenv("PGVECTOR_PASSWORD"),
    host=os.getenv("PGVECTOR_HOST"),
    port=os.getenv("PGVECTOR_PORT"),
    database=os.getenv("PGVECTOR_DATABASE"),
)

# Create the PGVector instance from the documents
db = PGVector.from_documents(
                                embedding=embeddings,
                                documents=docs,
                                collection_name=COLLECTION_NAME,
                                connection_string=CONNECTION_STRING,
                                use_jsonb = True
                            )





db.similarity_search("Can you detail the speaker's journey from starting as a coder to becoming a successful entrepreneur, including the pivot in their business model?")[:3]





chain = (
    {"context": db.as_retriever(), "question": RunnablePassthrough()}
    | prompt
    | model
    | parser
)

response = chain.invoke("What are the main challenges and advantages of doing business in India, including insights on market sensitivity, price, and speed of decision-making?")


print(response)



